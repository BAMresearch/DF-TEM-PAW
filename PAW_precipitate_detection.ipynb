{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d41620a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as thomas.hanke\n",
      "User ID: 102\n",
      "User Full Name: Thomas Hanke\n",
      "Your Groups:\n",
      "   Name: matolab  ID: 53\n",
      "   Name: kupferdigital  ID: 54\n",
      "Current group:  matolab\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import struct\n",
    "from omero.model.enums import UnitsLength\n",
    "from omero.model import RoiI, MaskI, ImageI, PolygonI, LengthI\n",
    "from omero.gateway import BlitzGateway, rint, rdouble, rstring\n",
    "from getpass import getpass\n",
    "import omero.clients\n",
    "from scipy import ndimage\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import regionprops_table\n",
    "from skimage.color import label2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "\n",
    "# csvtocsvw_url='https://csvtocsvw.matolab.org'\n",
    "# maptomethod_url='https://maptomethod.matolab.org'\n",
    "# rdfconverter_url='https://rdfconverter.matolab.org'\n",
    "# using private dev apps for now\n",
    "csvtocsvw_url = 'http://docker-dev.iwm.fraunhofer.de:5001'\n",
    "maptomethod_url = 'http://docker-dev.iwm.fraunhofer.de:5002'\n",
    "rdfconverter_url = 'http://docker-dev.iwm.fraunhofer.de:5003'\n",
    "\n",
    "\n",
    "def post_request(url, headers, data, files=None):\n",
    "    try:\n",
    "        if files:\n",
    "            # should crate a multipart form upload\n",
    "            response = requests.post(\n",
    "                url, data=data, headers=headers, files=files)\n",
    "        else:\n",
    "            # a application json post request\n",
    "            response = requests.post(\n",
    "                url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # placeholder for save file / clean-up\n",
    "        raise SystemExit(e) from None\n",
    "    return response\n",
    "\n",
    "\n",
    "def annotate_csv_uri(csv_url: str, encoding: str = 'auto',):\n",
    "    # curl -X 'POST' \\ 'https://csvtocsvw.matolab.org/api/annotation' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data_url\": \"https://github.com/Mat-O-Lab/CSVToCSVW/raw/main/examples/example.csv\", \"separator\": \"auto\", \"header_separator\": \"auto\", \"encoding\": \"auto\" }'\n",
    "    url = csvtocsvw_url+\"/api/annotate\"\n",
    "    data = {\n",
    "        \"data_url\": csv_url,\n",
    "        \"encoding\": encoding\n",
    "    }\n",
    "    headers = {'Content-type': 'application/json',\n",
    "               'Accept': 'application/json'}\n",
    "    r = post_request(url, headers, data).json()\n",
    "    filename = r['filename']\n",
    "    file = json.dumps(r['filedata'], indent=4).encode('utf-8')\n",
    "    print('csvw annotation file created, suggested name: {}'.format(filename))\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(file)\n",
    "        print('wrote csvw meta data to {}'.format(filename))\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def annotate_csv_upload(filepath: str, encoding: str = 'auto',):\n",
    "    # curl -X 'POST' \\ 'https://csvtocsvw.matolab.org/api/annotate_upload?encoding=auto' \\ -H 'accept: application/json' \\ -H 'Content-Type: multipart/form-data' \\ -F 'file=@detection_runs.csv;type=text/csv'\n",
    "    url = csvtocsvw_url+\"/api/annotate_upload?encoding=auto\"\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "    head, tail = os.path.split(filepath)\n",
    "    files = {\"file\": (tail, open(filepath, \"rb\"), \"text/csv\")}\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "def csvw_to_rdf(meta_url: str, format: str = 'turtle',):\n",
    "    # curl -X 'POST' \\ 'https://csvtocsvw.matolab.org/api/rdf' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"metadata_url\": \"https://github.com/Mat-O-Lab/resources/raw/main/rdfconverter/tests/detection_runs-metadata.json\", \"format\": \"turtle\" }'\n",
    "    url = csvtocsvw_url+\"/api/rdf\"\n",
    "    data = {\n",
    "        \"metadata_url\": meta_url,\n",
    "        \"format\": format\n",
    "    }\n",
    "    headers = {'Content-type': 'application/json',\n",
    "               'Accept': 'application/json'}\n",
    "    #r = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "    r = post_request(url, headers, data)\n",
    "    if r.status_code == 200:\n",
    "        d = r.headers['content-disposition']\n",
    "        fname = re.findall(\"filename=(.+)\", d)[0]\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print('writen serialized table to {}'.format(fname))\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_mapping(meta_url: str, method_url: str, map_dict: dict, data_super_classes: list, predicate: str, method_super_classes: list):\n",
    "    # curl -X 'POST' \\ 'https://csvtocsvw.matolab.org/api/annotation' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data_url\": \"https://github.com/Mat-O-Lab/CSVToCSVW/raw/main/examples/example.csv\", \"separator\": \"auto\", \"header_separator\": \"auto\", \"encoding\": \"auto\" }'\n",
    "    url = maptomethod_url+\"/api/mapping\"\n",
    "    data = {\n",
    "        \"data_url\": meta_url,\n",
    "        \"method_url\": method_url,\n",
    "        \"data_super_classes\": data_super_classes,\n",
    "        \"predicate\": predicate,\n",
    "        \"method_super_classes\": method_super_classes,\n",
    "        \"map\": map_dict\n",
    "    }\n",
    "    headers = {'Content-type': 'application/json',\n",
    "               'Accept': 'application/json'}\n",
    "    r = post_request(url, headers, data)\n",
    "    if r.status_code == 200:\n",
    "        d = r.headers['content-disposition']\n",
    "        fname = re.findall(\"filename=(.+)\", d)[0]\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print('writen mapping file to {}'.format(fname))\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_joined_rdf(map_url: str, data_url: str, duplicate_for_table=False):\n",
    "    # curl -X 'POST' \\ 'https://csvtocsvw.matolab.org/api/annotation' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"data_url\": \"https://github.com/Mat-O-Lab/CSVToCSVW/raw/main/examples/example.csv\", \"separator\": \"auto\", \"header_separator\": \"auto\", \"encoding\": \"auto\" }'\n",
    "    url = rdfconverter_url+\"/api/createrdf\"\n",
    "    data = {\n",
    "        \"mapping_url\": map_url,\n",
    "        \"data_url\": data_url,\n",
    "        \"duplicate_for_table\": duplicate_for_table\n",
    "    }\n",
    "    headers = {'Content-type': 'application/json',\n",
    "               'Accept': 'application/json'}\n",
    "    r = post_request(url, headers, data)\n",
    "    if r.status_code == 200:\n",
    "        r=r.json()\n",
    "        filename=r['filename']\n",
    "        print(\"applied {} mapping rules and skipped {}\".format(r['num_mappings_applied'],r['num_mappings_skipped']))\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(r['graph'])\n",
    "            print('wrote joint graph to {}'.format(filename))\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "\n",
    "# need credentials\n",
    "username = input(\"Username: \")\n",
    "password = getpass(\"OMERO Password: \")\n",
    "\n",
    "meta_extractor_api = \"https://metadata.omero.matolab.org/api/\"\n",
    "\n",
    "HOST = 'wss://wss.omero.matolab.org'\n",
    "client = omero.client(HOST)\n",
    "\n",
    "\n",
    "def test_connect():\n",
    "    session = client.createSession(username, password)\n",
    "    with BlitzGateway(client_obj=client) as conn:\n",
    "        print(\"Connected as {}\".format(conn.getUser().getName()))\n",
    "        print(\"User ID: {}\".format(conn.getUser().getId()))\n",
    "        print(\"User Full Name: {}\".format(conn.getUser().getFullName()))\n",
    "\n",
    "        print(\"Your Groups:\")\n",
    "        for g in conn.getGroupsMemberOf():\n",
    "            print(\"   Name:\", g.getName(), \" ID:\", g.getId())\n",
    "        group = conn.getGroupFromContext()\n",
    "        print(\"Current group: \", group.getName())\n",
    "\n",
    "\n",
    "def get_datasets():\n",
    "    session = client.createSession(username, password)\n",
    "    with BlitzGateway(client_obj=client) as conn:\n",
    "        # demo project\n",
    "        project = conn.getObject(\"Project\", 51)\n",
    "        return list(project.listChildren())\n",
    "\n",
    "# Load images in a specified dataset method\n",
    "\n",
    "\n",
    "def get_images(dataset_id):\n",
    "    \"\"\"\n",
    "    Load the images in the specified dataset\n",
    "    :param conn: The BlitzGateway\n",
    "    :param dataset_id: The dataset's id\n",
    "    :return: The Images or None\n",
    "    \"\"\"\n",
    "    session = client.createSession(username, password)\n",
    "    with BlitzGateway(client_obj=client) as conn:\n",
    "        dataset = conn.getObject(\"Dataset\", dataset_id)\n",
    "        images = []\n",
    "        if dataset:\n",
    "            for image in dataset.listChildren():\n",
    "                images.append(image)\n",
    "            if len(images) == 0:\n",
    "                return None\n",
    "\n",
    "        for image in images:\n",
    "            print(\"---- Loaded image ID:\", image.id)\n",
    "\n",
    "        # Print dataset ID and name\n",
    "        print(\"Dataset ID:\", dataset.getId())\n",
    "        print(\"Dataset Name:\", dataset.getName())\n",
    "    return images\n",
    "\n",
    "\n",
    "def create_mask(mask_bytes, bytes_per_pixel=1):\n",
    "    if bytes_per_pixel == 2:\n",
    "        divider = 16.0\n",
    "        format_string = \"H\"  # Unsigned short\n",
    "        byte_factor = 0.5\n",
    "    elif bytes_per_pixel == 1:\n",
    "        divider = 8.0\n",
    "        format_string = \"B\"  # Unsigned char\n",
    "        byte_factor = 1\n",
    "    else:\n",
    "        message = \"Format %s not supported\"\n",
    "        raise ValueError(message)\n",
    "    steps = math.ceil(len(mask_bytes) / divider)\n",
    "    mask = []\n",
    "    for i in range(int(steps)):\n",
    "        binary = mask_bytes[\n",
    "            i * int(divider):i * int(divider) + int(divider)]\n",
    "        format = str(int(byte_factor * len(binary))) + format_string\n",
    "        binary = struct.unpack(format, binary)\n",
    "        s = \"\"\n",
    "        for bit in binary:\n",
    "            s += str(bit)\n",
    "        mask.append(int(s, 2))\n",
    "    return bytearray(mask)\n",
    "\n",
    "# We have a helper function for creating an ROI and linking it to new shapes\n",
    "\n",
    "\n",
    "def create_roi(img_id, shapes):\n",
    "    # create an ROI, link it to Image\n",
    "    roi = RoiI()\n",
    "    # use the omero.model.ImageI that underlies the 'image' wrapper\n",
    "    session = client.createSession(username, password)\n",
    "    with BlitzGateway(client_obj=client) as conn:\n",
    "        img = conn.getObject(\"Image\", img_id)\n",
    "        roi.setImage(ImageI(img_id, False))\n",
    "        for shape in shapes:\n",
    "            roi.addShape(shape)\n",
    "        # Save the ROI (saves any linked shapes too)\n",
    "        updateService = conn.getUpdateService()\n",
    "        return updateService.saveAndReturnObject(roi)\n",
    "\n",
    "\n",
    "def delete_rois(image_id):\n",
    "    session = client.createSession(username, password)\n",
    "    with BlitzGateway(client_obj=client) as conn:\n",
    "        roi_service = conn.getRoiService()\n",
    "        result = roi_service.findByImage(image_id, None)\n",
    "        roi_ids = [roi.id.val for roi in result.rois]\n",
    "        if roi_ids:\n",
    "            conn.deleteObjects(\"Roi\", roi_ids)\n",
    "        print(\"deleted rois: {}\".format(roi_ids))\n",
    "\n",
    "\n",
    "def rgba_to_int(red, green, blue, alpha=255):\n",
    "    \"\"\" Return the color as an Integer in RGBA encoding \"\"\"\n",
    "    return int.from_bytes([red, green, blue, alpha],\n",
    "                          byteorder='big', signed=True)\n",
    "\n",
    "\n",
    "def print_image_details(img_id):\n",
    "    # Retrieve information about the image\n",
    "    session = client.createSession(username, password)\n",
    "    with BlitzGateway(client_obj=client) as conn:\n",
    "        image = conn.getObject(\"Image\", img_id)\n",
    "        print(\"Image Name:\", image.getName())\n",
    "        print(\"Image Name:\", image.id)\n",
    "        print(\"Image Description:\", image.getDescription())\n",
    "        print(\"Image SizeX:\", image.getSizeX())\n",
    "        print(\"Image SizeY:\", image.getSizeY())\n",
    "        print(\"Image SizeZ:\", image.getSizeZ())\n",
    "        print(\"Image SizeC:\", image.getSizeC())\n",
    "        print(\"Image SizeT:\", image.getSizeT())\n",
    "\n",
    "        x = image.getName()\n",
    "\n",
    "        # List Channels (loads the Rendering settings to get channel colors)\n",
    "        for channel in image.getChannels():\n",
    "            print('Channel:', channel.getLabel())\n",
    "            print('Color:', channel.getColor().getRGB())\n",
    "            print('Lookup table:', channel.getLut())\n",
    "            print('Is reverse intensity?', channel.isReverseIntensity())\n",
    "\n",
    "        print(image.countImportedImageFiles())\n",
    "        file_count = image.countFilesetFiles()\n",
    "        # list files\n",
    "        if file_count > 0:\n",
    "            for orig_file in image.getImportedImageFiles():\n",
    "                name = orig_file.getName()\n",
    "                path = orig_file.getPath()\n",
    "                print(name)\n",
    "                print(path)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_grayscale(img_id):\n",
    "    session = client.createSession(username, password)\n",
    "    with BlitzGateway(client_obj=client) as conn:\n",
    "        image = conn.getObject(\"Image\", img_id)\n",
    "        z = image.getSizeZ() / 2\n",
    "        t = 0\n",
    "        rendered_image = image.renderImage(z, t)\n",
    "        image_array = np.asarray(rendered_image)\n",
    "        # Convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "    return gray\n",
    "\n",
    "\n",
    "def create_omero_roi_polygons(image_id, contours):\n",
    "    the_t = 0\n",
    "    the_z = 0\n",
    "    polygons = []\n",
    "    # create an ROI with a single polygon\n",
    "    for i, contour in enumerate(contours):\n",
    "        polygon = PolygonI()\n",
    "        polygon.theZ = rint(the_z)\n",
    "        polygon.theT = rint(the_t)\n",
    "        color = list(np.random.choice(range(256), size=3))\n",
    "        polygon.strokeColor = rint(rgba_to_int(color[0], color[1], color[2]))\n",
    "        polygon.fillColor = rint(rgba_to_int(color[0], color[1], color[2]))\n",
    "\n",
    "        polygon.strokeWidth = LengthI(1, UnitsLength.PIXEL)\n",
    "        pts = [\"{},{}\".format(point[:, 0][0], point[:, 1][0])\n",
    "               for point in contour]\n",
    "        pts_list = \" \".join(pts)\n",
    "        polygon.points = rstring(pts_list)\n",
    "        # print(pts_list[:30])\n",
    "        polygon.textValue = rstring(\"Precipitate\"+str(i))\n",
    "        polygons.append(polygon)\n",
    "    # print(polygon.info())\n",
    "    # print(polygon.__dir__())\n",
    "    create_roi(image_id, polygons)\n",
    "    print(\"added {} polygon shapes to image\".format(len(polygons)))\n",
    "\n",
    "\n",
    "def run_detection(\n",
    "        image,\n",
    "        threshold_method: str = \"Otsu\",\n",
    "        size_thresh: float = 80,\n",
    "        dilate_kernel_size: int = 3,\n",
    "        median_filter_radius: int = 4,\n",
    "        save_to_file=False,\n",
    "        plot=False\n",
    "):\n",
    "\n",
    "    name = image.name\n",
    "    print(\"Image Name: \"+name)\n",
    "\n",
    "    gray = get_grayscale(image.id)\n",
    "\n",
    "    print(\"Grayscale image shape:\", gray.shape)\n",
    "    print(\"Grayscale image data type:\", gray.dtype)\n",
    "\n",
    "    # Apply median filter using OpenCV\n",
    "\n",
    "    #selem = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2*median_filter_radius + 1, 2*median_filter_radius+1))\n",
    "    filtered = cv2.medianBlur(gray, 2 * median_filter_radius + 1)\n",
    "\n",
    "    # Display the original and filtered images side by side\n",
    "    if plot:\n",
    "        fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax0.imshow(gray, cmap='gray')\n",
    "        ax0.set_title('Step1: Load Original Image')\n",
    "        ax0.axis('off')\n",
    "        ax1.imshow(filtered, cmap='gray')\n",
    "        ax1.set_title('Step2: Median Blur Filter')\n",
    "        ax1.axis('off')\n",
    "\n",
    "    # Apply Otsu's thresholding using OpenCV\n",
    "    if threshold_method == \"Otsu\":\n",
    "        ret, thresh = cv2.threshold(\n",
    "            filtered, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        print('Threshold value is {}'.format(ret))\n",
    "    else:\n",
    "        print(\"only Otsu thresholding supported, skippping thresholding\")\n",
    "        thresh = gray\n",
    "\n",
    "    # Dilate the thresholded image using a 3x3 kernel\n",
    "    kernel = np.ones((dilate_kernel_size, dilate_kernel_size), np.uint8)\n",
    "    dilated = cv2.dilate(thresh, kernel)\n",
    "\n",
    "    if plot:\n",
    "        fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax0.imshow(thresh, cmap='gray')\n",
    "        ax0.set_title('Step3: Otsu Threshold')\n",
    "        ax0.axis('off')\n",
    "        ax1.imshow(dilated, cmap='gray')\n",
    "        ax1.set_title('Step4: Dilated Image')\n",
    "        ax1.axis('off')\n",
    "\n",
    "    # Remove small objects using OpenCV's morphologyEx function\n",
    "    morphed = cv2.morphologyEx(dilated, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    contours, hierarchy = cv2.findContours(\n",
    "        morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area < size_thresh:\n",
    "            cv2.drawContours(morphed, [cnt], 0, 0, -1)\n",
    "\n",
    "    # delete all previously defined shapes\n",
    "    delete_rois(image.id)\n",
    "\n",
    "    # upload shapes to omero\n",
    "    create_omero_roi_polygons(image.id, contours)\n",
    "\n",
    "    # Apply clear border to the dilated image\n",
    "    mask = morphed == 255\n",
    "    mask = clear_border(mask)\n",
    "\n",
    "    # Label the mask and count the number of precipitates detected\n",
    "    s = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\n",
    "    labeled_mask, num_labels = ndimage.label(mask, structure=s)\n",
    "    img2 = label2rgb(labeled_mask, bg_label=0)\n",
    "\n",
    "    # Display the labeled mask\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(img2)\n",
    "        ax.set_title('Step5: Contour Finding')\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "        print(f\"Number of precipitates detected: {num_labels}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_connect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "634f3b1f",
   "metadata": {},
   "source": [
    "## The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc57a6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190C_1000h\n",
      "---- Loaded image ID: 83\n",
      "---- Loaded image ID: 84\n",
      "---- Loaded image ID: 85\n",
      "---- Loaded image ID: 74\n",
      "---- Loaded image ID: 75\n",
      "---- Loaded image ID: 76\n",
      "---- Loaded image ID: 77\n",
      "---- Loaded image ID: 78\n",
      "---- Loaded image ID: 79\n",
      "---- Loaded image ID: 80\n",
      "---- Loaded image ID: 81\n",
      "---- Loaded image ID: 82\n",
      "Dataset ID: 53\n",
      "Dataset Name: 190C_1000h\n",
      "190C_25000h_S1\n",
      "---- Loaded image ID: 142\n",
      "---- Loaded image ID: 143\n",
      "---- Loaded image ID: 133\n",
      "---- Loaded image ID: 134\n",
      "---- Loaded image ID: 135\n",
      "---- Loaded image ID: 136\n",
      "---- Loaded image ID: 137\n",
      "---- Loaded image ID: 138\n",
      "---- Loaded image ID: 139\n",
      "---- Loaded image ID: 140\n",
      "---- Loaded image ID: 141\n",
      "Dataset ID: 58\n",
      "Dataset Name: 190C_25000h_S1\n",
      "190C_25000h_S2\n",
      "---- Loaded image ID: 153\n",
      "---- Loaded image ID: 144\n",
      "---- Loaded image ID: 145\n",
      "---- Loaded image ID: 146\n",
      "---- Loaded image ID: 147\n",
      "---- Loaded image ID: 148\n",
      "---- Loaded image ID: 149\n",
      "---- Loaded image ID: 150\n",
      "---- Loaded image ID: 151\n",
      "---- Loaded image ID: 152\n",
      "Dataset ID: 59\n",
      "Dataset Name: 190C_25000h_S2\n",
      "190C_2500h_S1\n",
      "---- Loaded image ID: 86\n",
      "---- Loaded image ID: 87\n",
      "---- Loaded image ID: 88\n",
      "---- Loaded image ID: 89\n",
      "---- Loaded image ID: 90\n",
      "Dataset ID: 54\n",
      "Dataset Name: 190C_2500h_S1\n",
      "190C_2500h_S2\n",
      "---- Loaded image ID: 100\n",
      "---- Loaded image ID: 101\n",
      "---- Loaded image ID: 102\n",
      "---- Loaded image ID: 91\n",
      "---- Loaded image ID: 92\n",
      "---- Loaded image ID: 93\n",
      "---- Loaded image ID: 94\n",
      "---- Loaded image ID: 95\n",
      "---- Loaded image ID: 96\n",
      "---- Loaded image ID: 97\n",
      "---- Loaded image ID: 98\n",
      "---- Loaded image ID: 99\n",
      "Dataset ID: 55\n",
      "Dataset Name: 190C_2500h_S2\n",
      "190C_250h\n",
      "---- Loaded image ID: 72\n",
      "---- Loaded image ID: 73\n",
      "---- Loaded image ID: 63\n",
      "---- Loaded image ID: 64\n",
      "---- Loaded image ID: 65\n",
      "---- Loaded image ID: 66\n",
      "---- Loaded image ID: 67\n",
      "---- Loaded image ID: 68\n",
      "---- Loaded image ID: 69\n",
      "---- Loaded image ID: 70\n",
      "---- Loaded image ID: 71\n",
      "Dataset ID: 52\n",
      "Dataset Name: 190C_250h\n",
      "190C_5000h_S1\n",
      "---- Loaded image ID: 112\n",
      "---- Loaded image ID: 113\n",
      "---- Loaded image ID: 103\n",
      "---- Loaded image ID: 104\n",
      "---- Loaded image ID: 105\n",
      "---- Loaded image ID: 106\n",
      "---- Loaded image ID: 107\n",
      "---- Loaded image ID: 108\n",
      "---- Loaded image ID: 111\n",
      "---- Loaded image ID: 109\n",
      "---- Loaded image ID: 110\n",
      "Dataset ID: 56\n",
      "Dataset Name: 190C_5000h_S1\n",
      "190C_5000h_S2\n",
      "---- Loaded image ID: 132\n",
      "---- Loaded image ID: 128\n",
      "---- Loaded image ID: 129\n",
      "---- Loaded image ID: 119\n",
      "---- Loaded image ID: 120\n",
      "---- Loaded image ID: 121\n",
      "---- Loaded image ID: 122\n",
      "---- Loaded image ID: 123\n",
      "---- Loaded image ID: 124\n",
      "---- Loaded image ID: 125\n",
      "---- Loaded image ID: 126\n",
      "---- Loaded image ID: 127\n",
      "Dataset ID: 57\n",
      "Dataset Name: 190C_5000h_S2\n",
      "190C_8760h_s1\n",
      "---- Loaded image ID: 256\n",
      "---- Loaded image ID: 257\n",
      "---- Loaded image ID: 252\n",
      "---- Loaded image ID: 258\n",
      "---- Loaded image ID: 253\n",
      "---- Loaded image ID: 259\n",
      "---- Loaded image ID: 254\n",
      "---- Loaded image ID: 260\n",
      "---- Loaded image ID: 255\n",
      "---- Loaded image ID: 261\n",
      "---- Loaded image ID: 251\n",
      "Dataset ID: 101\n",
      "Dataset Name: 190C_8760h_s1\n",
      "190C_8760h_s2\n",
      "---- Loaded image ID: 262\n",
      "---- Loaded image ID: 263\n",
      "---- Loaded image ID: 264\n",
      "---- Loaded image ID: 265\n",
      "---- Loaded image ID: 266\n",
      "---- Loaded image ID: 267\n",
      "---- Loaded image ID: 268\n",
      "---- Loaded image ID: 269\n",
      "Dataset ID: 102\n",
      "Dataset Name: 190C_8760h_s2\n",
      "T61\n",
      "---- Loaded image ID: 51\n",
      "---- Loaded image ID: 52\n",
      "---- Loaded image ID: 53\n",
      "---- Loaded image ID: 202\n",
      "---- Loaded image ID: 203\n",
      "---- Loaded image ID: 55\n",
      "---- Loaded image ID: 56\n",
      "---- Loaded image ID: 57\n",
      "---- Loaded image ID: 58\n",
      "---- Loaded image ID: 59\n",
      "---- Loaded image ID: 60\n",
      "---- Loaded image ID: 61\n",
      "---- Loaded image ID: 62\n",
      "Dataset ID: 51\n",
      "Dataset Name: T61\n"
     ]
    }
   ],
   "source": [
    "#load all images from selected datasets, run detection algorithm and upload found contours as polygons in a omero roi\n",
    "#results are saved as detection_runs.csv\n",
    "datasets=get_datasets()\n",
    "data=list()\n",
    "for dataset in datasets:\n",
    "    print(dataset.name)\n",
    "    images = get_images(dataset.id)\n",
    "    for image in images:\n",
    "        startTime = datetime.now().isoformat()\n",
    "        info = image.name.split(\".\",1)[0].split('_')\n",
    "        state=info[0].split(\"-\")\n",
    "        if len(state)>1:\n",
    "            anneal_temp, anneal_time = info[0].split(\"-\")\n",
    "            anneal_temp = float(anneal_temp.rsplit('C',1)[0])\n",
    "            anneal_time = float(anneal_time.rsplit('h',1)[0])\n",
    "        else:\n",
    "            anneal_temp, anneal_time = 0.0, 0.0\n",
    "        specimen = info[1].split('Sample',1)[-1]\n",
    "        pos = info[2].split('Sample',1)[-1]\n",
    "        threshold_method=\"Otsu\"\n",
    "        size_thresh=80\n",
    "        dilate_kernel_size=3\n",
    "        median_filter_radius=4\n",
    "        info_dict={\n",
    "            \"SpecimenName\": dataset.name+'_'+specimen,\n",
    "            \"Aging Temp [°C]\": float(anneal_temp),\n",
    "            \"Aging Time [h]\": float(anneal_time),\n",
    "            \"Creep Stress [MPa]\": 0,\n",
    "            \"Dataset\": meta_extractor_api+\"dataset/\"+str(dataset.id),\n",
    "            \"Position_Id\": pos,\n",
    "            \"Image\": meta_extractor_api+\"image/\"+str(image.id),\n",
    "            \"ROIs\": meta_extractor_api+\"rois/\"+str(image.id),\n",
    "            \"DiskRadiusValue [px]\": median_filter_radius,\n",
    "            \"Threshold Method\": threshold_method,\n",
    "            \"Dilation Kernel Size [px]\": dilate_kernel_size,\n",
    "            \"Date\": startTime\n",
    "        }\n",
    "        run_detection(image,threshold_method=\"Otsu\",size_thresh=80,dilate_kernel_size=3,median_filter_radius=4)\n",
    "        data.append(info_dict)\n",
    "    #     break\n",
    "    # break\n",
    "df=pd.DataFrame(data)\n",
    "df.to_csv('detection_runs.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b226c24",
   "metadata": {},
   "source": [
    "# Processing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cfe3ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csvw annotation file created, suggested name: detection_runs-metadata.json\n",
      "wrote csvw meta data to detection_runs-metadata.json\n"
     ]
    }
   ],
   "source": [
    "# annotate detection_runs.csv\n",
    "response=annotate_csv_uri(\"https://github.com/BAMresearch/DF-TEM-PAW/raw/main/detection_runs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db349af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writen serialized table to detection_runs.ttl\n"
     ]
    }
   ],
   "source": [
    "# serialize table to rdf, uses already commited files on main branch\n",
    "meta_url=\"https://github.com/BAMresearch/DF-TEM-PAW/raw/main/detection_runs-metadata.json\"\n",
    "response=csvw_to_rdf(meta_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2cb82cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writen mapping file to detection_runs-map.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a rule bases mapping between the data in detection_runs and the precipitate analysis knowledge graph \n",
    "meta_url=\"https://github.com/BAMresearch/DF-TEM-PAW/raw/main/detection_runs-metadata.json\"\n",
    "method_url=\"https://github.com/BAMresearch/DF-TEM-PAW/raw/main/PrecipitateAnalysisWorkflow.ttl\"\n",
    "d_classes= [\n",
    "    \"http://www.w3.org/ns/oa#Annotation\",''\n",
    "    \"http://www.w3.org/ns/csvw#Column\"\n",
    "]\n",
    "m_classes=[\"https://w3id.org/pmd/co/Metadata\",]\n",
    "pred=\"https://w3id.org/pmd/co/isResourceOf\"\n",
    "map_dict={\n",
    "    \"diskRadius_\": \"table-1-DiskradiusvaluePx\",\n",
    "    \"kernel_\": \"table-1-DilationKernelSizePx\",\n",
    "    \"thresholdMethod_\": \"table-1-ThresholdMethod\",\n",
    "    \"specimenAgingTemperature_\": \"table-1-AgingTempC\",\n",
    "    \"specimenAgingTime_\": \"table-1-DiskradiusvaluePx\",\n",
    "    \"specimenCreepStess_\": \"table-1-CreepStressMpa\",\n",
    "    \"specimenName_\": \"table-1-Specimenname\",\n",
    "    \"specimenAgingTime_\": \"table-1-AgingTimeH\",\n",
    "    \"imageDataset_\":\"table-1-Dataset\",\n",
    "    \"darkfieldTransmissionElectronMicroscopeImage_\": \"table-1-Image\",\n",
    "}\n",
    "create_mapping(meta_url=meta_url,method_url=method_url,data_super_classes=d_classes,predicate=pred,method_super_classes=m_classes,map_dict=map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f46d6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applied 7 mapping rules and skipped 0\n",
      "wrote joint graph to detection_runs-joined.ttl\n"
     ]
    }
   ],
   "source": [
    "# join all data and replicate template knowledge graph for every row in table\n",
    "mapping_url = \"https://github.com/BAMresearch/DF-TEM-PAW/raw/main/detection_runs-map.yaml\"\n",
    "data_url = \"https://github.com/BAMresearch/DF-TEM-PAW/raw/main/detection_runs.ttl\"\n",
    "duplicate_for_table = True\n",
    "get_joined_rdf(map_url=mapping_url,data_url=data_url,duplicate_for_table=duplicate_for_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv_skimage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "62618c75dbbeddb9025e5d06afcb46d4db33ffb9a9c357e8bdf42838eaa81943"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
